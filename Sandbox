Avec pipeline pour séparer par locuteur : 

**code** 
=======================================================================================
from torch import cuda
cuda.get_device_name(0)

!pip install git+https://github.com/openai/whisper.git
from torch import cuda
cuda.get_device_name(0)
print("Veuillez déposer votre fichier audio :")

from google.colab import files 
uploaded = files.upload()
audio_path = list(uploaded.keys())[0]
print(f"Fichier reçu : {audio_path}")

import whisper

puissance = "medium"  # @param ["base", "medium", "turbo", "large-v3"]
def transcribe_with_speaker_changes(video_path, model_size=(puissance)):
   
    model = whisper.load_model(model_size)

    result = model.transcribe(video_path, fp16=False)

 
    transcription = ""
    last_speaker = None

    for segment in result['segments']:
        start_time = segment['start']
        end_time = segment['end']
        text = segment['text']

      
        if last_speaker is None or start_time - last_speaker > 2:  # Ajuster si besoin
            transcription += "\n"  

        transcription += text.strip() + " "
        last_speaker = end_time

    return transcription.strip()

if __name__ == "__main__":
    video_path = (audio_path)  
    output = transcribe_with_speaker_changes(video_path)

    with open("transcription.txt", "w", encoding="utf-8") as file:
        file.write(output)

    print("Transcription completed and saved to 'transcription.txt'.")

=======================================================================================

Ce code fonctionne (enfin!!!). Voir si beaucoup d'erreurs dans le saut de ligne. 

======================================================================================

!pip uninstall torch torchaudio nemo-toolkit -y
!pip install wget
!apt-get install sox libsndfile1 ffmpeg
!pip install text-unidecode
BRANCH = 'r2.0.0rc0'
!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[asr]
!pip install torchaudio -f https://download.pytorch.org/whl/torch_stable.html




import os
import json
import whisper
import torch
from google.colab import files
from nemo.collections.asr.models import ClusteringDiarize

# Vérification du GPU
if torch.cuda.is_available():
    print(f"GPU disponible : {torch.cuda.get_device_name(0)}")
else:
    print("GPU non disponible, exécution sur CPU.")

# Étape 1 : Chargement du fichier audio
print("Veuillez déposer votre fichier audio :")
uploaded = files.upload()
audio_path = list(uploaded.keys())[0]
print(f"Fichier reçu : {audio_path}")

# ... (rest of your code)

# Étape 2 : Configuration de la diarization (NeMo)
manifest_path = "manifest.json"
diarization_output_path = "diarization_output/pred_rttms"

# Création du manifest pour NeMo
manifest_content = {
    "audio_filepath": audio_path,
    "offset": 0,  # Décalage de début (en secondes)
    "duration": None,  # Durée complète
    "label": "infer",  # Mode d'inférence
    "text": "-",  # Texte par défaut
    "num_speakers": None,  # Détection automatique du nombre de locuteurs
}
with open(manifest_path, "w") as f:
    json.dump(manifest_content, f)

# Initialisation du modèle de diarization
diarizer = ClusteringDiarizer(cfg={
    "manifest_filepath": manifest_path,
    "out_dir": "diarization_output",
    "vad_model_path": "vad_multilingual_marblenet",
    "speaker_embeddings_model_path": "titanet_large",
    "oracle_vad": False,  # Détection automatique des segments vocaux
    "clustering_params": {
        "threshold": 0.3,  # Ajuster pour équilibrer précision et rappel
    },
})

print("Diarization en cours...")
diarizer.diarize()

# Vérifier si le fichier RTTM est généré
if not os.path.exists(diarization_output_path):
    raise FileNotFoundError(f"Le fichier RTTM n'a pas été généré dans : {diarization_output_path}")
else:
    print("Diarization terminée. Résultats sauvegardés dans le fichier RTTM.")

# Charger les résultats RTTM
rttm_path = os.path.join("diarization_output", "pred_rttms", os.listdir("diarization_output/pred_rttms")[0])
speaker_segments = []
with open(rttm_path, "r") as rttm_file:
    for line in rttm_file:
        parts = line.strip().split()
        start_time = float(parts[3])
        duration = float(parts[4])
        speaker = parts[7]
        end_time = start_time + duration
        speaker_segments.append({"start": start_time, "end": end_time, "speaker": speaker})

# Étape 3 : Transcription avec Whisper
model_size = "medium"  # Taille du modèle Whisper (modifiable)
model = whisper.load_model(model_size)

print("Transcription en cours...")
whisper_result = model.transcribe(audio_path, fp16=torch.cuda.is_available())
transcription_segments = whisper_result['segments']

# Étape 4 : Fusion des résultats Whisper + NeMo
final_transcription = ""
current_speaker = None

for segment in transcription_segments:
    segment_start = segment['start']
    segment_end = segment['end']
    text = segment['text']

    # Trouver le locuteur correspondant dans les segments de NeMo
    for speaker_segment in speaker_segments:
        if segment_start >= speaker_segment['start'] and segment_end <= speaker_segment['end']:
            speaker = speaker_segment['speaker']
            break
    else:
        speaker = "Inconnu"  # Si aucun locuteur n'est trouvé (cas rare)

    # Ajouter un changement de locuteur si nécessaire
    if speaker != current_speaker:
        final_transcription += f"\n[{speaker}]\n"
        current_speaker = speaker

    # Ajouter le texte transcrit
    final_transcription += text.strip() + " "

# Étape 5 : Sauvegarde de la transcription
output_path = "transcription_avec_locuteurs.txt"
with open(output_path, "w", encoding="utf-8") as f:
    f.write(final_transcription.strip())

print(f"Transcription finalisée et sauvegardée dans : {output_path}")
