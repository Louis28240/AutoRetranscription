Avec pipeline pour séparer par locuteur : 

**code** 
=======================================================================================
from torch import cuda
cuda.get_device_name(0)

!pip install git+https://github.com/openai/whisper.git
from torch import cuda
cuda.get_device_name(0)
print("Veuillez déposer votre fichier audio :")

from google.colab import files 
uploaded = files.upload()
audio_path = list(uploaded.keys())[0]
print(f"Fichier reçu : {audio_path}")

import whisper

puissance = "medium"  # @param ["base", "medium", "turbo", "large-v3"]
def transcribe_with_speaker_changes(video_path, model_size=(puissance)):
   
    model = whisper.load_model(model_size)

    result = model.transcribe(video_path, fp16=False)

 
    transcription = ""
    last_speaker = None

    for segment in result['segments']:
        start_time = segment['start']
        end_time = segment['end']
        text = segment['text']

      
        if last_speaker is None or start_time - last_speaker > 2:  # Ajuster si besoin
            transcription += "\n"  

        transcription += text.strip() + " "
        last_speaker = end_time

    return transcription.strip()

if __name__ == "__main__":
    video_path = (audio_path)  
    output = transcribe_with_speaker_changes(video_path)

    with open("transcription.txt", "w", encoding="utf-8") as file:
        file.write(output)

    print("Transcription completed and saved to 'transcription.txt'.")

=======================================================================================

Ce code fonctionne (enfin!!!). Voir si beaucoup d'erreurs dans le saut de ligne. 
