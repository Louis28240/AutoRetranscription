Avec pipeline pour séparer par locuteur : 

**code** 
=======================================================================================
!pip install git+https://github.com/openai/whisper.git
from torch import cuda
import whisper
from google.colab import files

if cuda.is_available():
    print(f"GPU disponible : {cuda.get_device_name(0)}")
else:
    print("GPU non disponible, utilisation du CPU.")

choix = "avec"  # @param ["avec", "sans"]
puissance = "large-v3"  # @param ["base", "medium", "turbo", "large-v3"]


locuteur_1 = "locuteur 1"  # Nom du premier locuteur
locuteur_2 = "locuteur 2"    # Nom du second locuteur

if choix == "avec":

    print("Veuillez déposer votre fichier audio :")
    uploaded = files.upload()
    audio_path = list(uploaded.keys())[0]
    print(f"Fichier reçu : {audio_path}")

    def transcribe_with_two_speakers(video_path, model_size, speaker1, speaker2):

        model = whisper.load_model(model_size)
        result = model.transcribe(video_path, fp16=False, word_timestamps=True)

        transcription = ""
        current_speaker = speaker1

        for segment in result['segments']:

            if current_speaker == speaker1:
                current_speaker = speaker2
            else:
                current_speaker = speaker1


            transcription += f"[{current_speaker}] {segment['text'].strip()}\n"

        return transcription.strip()


    transcription = transcribe_with_two_speakers(audio_path, puissance, locuteur_1, locuteur_2)


    with open("transcription.txt", "w", encoding="utf-8") as file:
        file.write(transcription)

    print("Transcription terminée et sauvegardée dans 'transcription.txt'.")

elif choix == "sans":
    print("Veuillez déposer votre fichier audio :")
    uploaded = files.upload()
    audio_path = list(uploaded.keys())[0]
    print(f"Fichier reçu : {audio_path}")

    !whisper --model {puissance} --output_format txt "{audio_path}"

    print("Option invalide. Veuillez choisir 'avec' ou 'sans'.")

=======================================================================================

Ce code fonctionne (enfin!!!). Voir si beaucoup d'erreurs dans le saut de ligne. 

======================================================================================

!pip uninstall torch torchaudio nemo-toolkit -y
!pip install wget
!apt-get install sox libsndfile1 ffmpeg
!pip install text-unidecode
BRANCH = 'r2.0.0rc0'
!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[asr]
!pip install torchaudio -f https://download.pytorch.org/whl/torch_stable.html




import os
import json
import whisper
import torch
from google.colab import files
from nemo.collections.asr.models import ClusteringDiarize

# Vérification du GPU
if torch.cuda.is_available():
    print(f"GPU disponible : {torch.cuda.get_device_name(0)}")
else:
    print("GPU non disponible, exécution sur CPU.")

# Étape 1 : Chargement du fichier audio
print("Veuillez déposer votre fichier audio :")
uploaded = files.upload()
audio_path = list(uploaded.keys())[0]
print(f"Fichier reçu : {audio_path}")

# ... (rest of your code)

# Étape 2 : Configuration de la diarization (NeMo)
manifest_path = "manifest.json"
diarization_output_path = "diarization_output/pred_rttms"

# Création du manifest pour NeMo
manifest_content = {
    "audio_filepath": audio_path,
    "offset": 0,  # Décalage de début (en secondes)
    "duration": None,  # Durée complète
    "label": "infer",  # Mode d'inférence
    "text": "-",  # Texte par défaut
    "num_speakers": None,  # Détection automatique du nombre de locuteurs
}
with open(manifest_path, "w") as f:
    json.dump(manifest_content, f)

# Initialisation du modèle de diarization
diarizer = ClusteringDiarizer(cfg={
    "manifest_filepath": manifest_path,
    "out_dir": "diarization_output",
    "vad_model_path": "vad_multilingual_marblenet",
    "speaker_embeddings_model_path": "titanet_large",
    "oracle_vad": False,  # Détection automatique des segments vocaux
    "clustering_params": {
        "threshold": 0.3,  # Ajuster pour équilibrer précision et rappel
    },
})

print("Diarization en cours...")
diarizer.diarize()

# Vérifier si le fichier RTTM est généré
if not os.path.exists(diarization_output_path):
    raise FileNotFoundError(f"Le fichier RTTM n'a pas été généré dans : {diarization_output_path}")
else:
    print("Diarization terminée. Résultats sauvegardés dans le fichier RTTM.")

# Charger les résultats RTTM
rttm_path = os.path.join("diarization_output", "pred_rttms", os.listdir("diarization_output/pred_rttms")[0])
speaker_segments = []
with open(rttm_path, "r") as rttm_file:
    for line in rttm_file:
        parts = line.strip().split()
        start_time = float(parts[3])
        duration = float(parts[4])
        speaker = parts[7]
        end_time = start_time + duration
        speaker_segments.append({"start": start_time, "end": end_time, "speaker": speaker})

# Étape 3 : Transcription avec Whisper
model_size = "medium"  # Taille du modèle Whisper (modifiable)
model = whisper.load_model(model_size)

print("Transcription en cours...")
whisper_result = model.transcribe(audio_path, fp16=torch.cuda.is_available())
transcription_segments = whisper_result['segments']

# Étape 4 : Fusion des résultats Whisper + NeMo
final_transcription = ""
current_speaker = None

for segment in transcription_segments:
    segment_start = segment['start']
    segment_end = segment['end']
    text = segment['text']

    # Trouver le locuteur correspondant dans les segments de NeMo
    for speaker_segment in speaker_segments:
        if segment_start >= speaker_segment['start'] and segment_end <= speaker_segment['end']:
            speaker = speaker_segment['speaker']
            break
    else:
        speaker = "Inconnu"  # Si aucun locuteur n'est trouvé (cas rare)

    # Ajouter un changement de locuteur si nécessaire
    if speaker != current_speaker:
        final_transcription += f"\n[{speaker}]\n"
        current_speaker = speaker

    # Ajouter le texte transcrit
    final_transcription += text.strip() + " "

# Étape 5 : Sauvegarde de la transcription
output_path = "transcription_avec_locuteurs.txt"
with open(output_path, "w", encoding="utf-8") as f:
    f.write(final_transcription.strip())

print(f"Transcription finalisée et sauvegardée dans : {output_path}")
